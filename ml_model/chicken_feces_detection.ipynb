{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1899f903",
   "metadata": {},
   "source": [
    "# Poultry Disease Classification from Fecal Images\n",
    "\n",
    "This notebook classifies poultry diseases based on fecal images into four classes:\n",
    "\n",
    "- **Healthy**\n",
    "- **Coccidiosis (cocci)**\n",
    "- **Salmonella (salmo)**\n",
    "- **Newcastle Disease (ncd)**\n",
    "\n",
    "> ‚öôÔ∏è *The model is optimized for Mac M1 with 8GB RAM.* (Farhan Mashrur)\n",
    "- Model initially developed with Ahmed Abdulla (Teammate) for Mac M2 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27fc4e4",
   "metadata": {},
   "source": [
    "### GPU availability Testing :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1f19079",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/farhanmashrur/Desktop/cds/avian_alert/tf-metal-env/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.2\n",
      "GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Num GPUs Available: 1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPUs:\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe6b22a",
   "metadata": {},
   "source": [
    " ## 1. Importing Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3bde5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam, Adamax\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout, BatchNormalization\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Enable mixed precision for Apple Silicon (M1/M2)\n",
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print('Modules loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811654e8",
   "metadata": {},
   "source": [
    "## 2. Enable GPU Acceleration (for Mac M1)\n",
    "#### For Mac M1, we are using TensorFlow-MacOS and Metal plugin are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a490c415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU acceleration enabled on M1\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    physical_devices = tf.config.list_physical_devices('GPU')\n",
    "    if len(physical_devices) > 0:\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "        print(\"GPU acceleration enabled on M1\")\n",
    "    else:\n",
    "        print(\"No GPU found\")\n",
    "except Exception as e:\n",
    "    print(\"GPU acceleration not available:\", e)\n",
    "\n",
    "\n",
    "# 3. Image Settings for Mac M1 (8GB RAM)\n",
    "IMG_SIZE = (160, 160)  # Reduced for efficiency\n",
    "CHANNELS = 3\n",
    "BATCH_SIZE = 32        # Lower to 16 if memory issues occur\n",
    "IMG_SHAPE = (IMG_SIZE[0], IMG_SIZE[1], CHANNELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b559392",
   "metadata": {},
   "source": [
    "## 3) Custom Callback for training and Monitoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6d2d274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "class MyCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, model, patience=1, stop_patience=3, threshold=0.9, factor=0.5, batches=None, epochs=None):\n",
    "        super(MyCallback, self).__init__()\n",
    "        self._model = model\n",
    "        self.patience = patience \n",
    "        self.stop_patience = stop_patience\n",
    "        self.threshold = threshold\n",
    "        self.factor = factor\n",
    "        self.batches = batches\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        self.count = 0\n",
    "        self.stop_count = 0\n",
    "        self.best_epoch = 1\n",
    "        \n",
    "        try:\n",
    "            self.current_lr = 0.001\n",
    "            if hasattr(model.optimizer, 'learning_rate'):\n",
    "                lr = model.optimizer.learning_rate\n",
    "                if hasattr(lr, 'numpy'):\n",
    "                    self.current_lr = float(lr.numpy())\n",
    "            elif hasattr(model.optimizer, 'lr'):\n",
    "                lr = model.optimizer.lr\n",
    "                if hasattr(lr, 'numpy'):\n",
    "                    self.current_lr = float(lr.numpy())\n",
    "        except:\n",
    "            self.current_lr = 0.001\n",
    "            \n",
    "        self.initial_lr = self.current_lr\n",
    "        self.highest_tracc = 0.0\n",
    "        self.lowest_vloss = np.inf\n",
    "        self.best_weights = self._model.get_weights()\n",
    "        self.initial_weights = self._model.get_weights()\n",
    "\n",
    "        print(\"‚úÖ Callback initialization complete.\")\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        msg = '{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:^9s}{7:^10s}{8:10s}{9:^8s}'.format(\n",
    "            'Epoch', 'Loss', 'Accuracy', 'V_loss', 'V_acc', 'LR', 'Next LR', 'Monitor','% Improv', 'Duration')\n",
    "        print(msg)\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        stop_time = time.time()\n",
    "        tr_duration = stop_time - self.start_time\n",
    "        hours = tr_duration // 3600\n",
    "        minutes = (tr_duration - (hours * 3600)) // 60\n",
    "        seconds = tr_duration - ((hours * 3600) + (minutes * 60))\n",
    "        msg = f'\\nTraining time: {int(hours)}h {int(minutes)}m {seconds:.2f}s'\n",
    "        print(msg)\n",
    "        self._model.set_weights(self.best_weights)\n",
    "        print(\"‚úÖ Best weights restored.\")\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        acc = logs.get('accuracy') * 100\n",
    "        loss = logs.get('loss')\n",
    "        msg = f'processing batch {batch + 1} of {self.batches} - accuracy: {acc:.2f}% - loss: {loss:.5f}'\n",
    "        print(msg, '\\r', end='')\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.ep_start = time.time()\n",
    "        print(f\"\\nüîÅ Starting epoch {epoch + 1}\")\n",
    "\n",
    "    def _update_lr(self, new_lr):\n",
    "        try:\n",
    "            if hasattr(self._model.optimizer, 'learning_rate'):\n",
    "                tf.keras.backend.set_value(self._model.optimizer.learning_rate, new_lr)\n",
    "            elif hasattr(self._model.optimizer, 'lr'):\n",
    "                tf.keras.backend.set_value(self._model.optimizer.lr, new_lr)\n",
    "            self.current_lr = new_lr\n",
    "            print(f\"üìâ Learning rate updated to {new_lr:.6f}\")\n",
    "        except Exception as e:\n",
    "            print(\"‚ö†Ô∏è Failed to update learning rate:\", e)\n",
    "        return new_lr\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        ep_end = time.time()\n",
    "        duration = ep_end - self.ep_start\n",
    "        current_lr = self.current_lr\n",
    "\n",
    "        acc = logs.get('accuracy')\n",
    "        v_acc = logs.get('val_accuracy')\n",
    "        loss = logs.get('loss')\n",
    "        v_loss = logs.get('val_loss')\n",
    "        next_lr = current_lr\n",
    "\n",
    "        if acc < self.threshold:\n",
    "            monitor = 'accuracy'\n",
    "            pimprov = 0.0 if epoch == 0 else (acc - self.highest_tracc) * 100 / self.highest_tracc\n",
    "            if acc > self.highest_tracc:\n",
    "                self.highest_tracc = acc\n",
    "                self.best_weights = self._model.get_weights()\n",
    "                self.count = 0\n",
    "                self.stop_count = 0\n",
    "                if v_loss < self.lowest_vloss:\n",
    "                    self.lowest_vloss = v_loss\n",
    "                self.best_epoch = epoch + 1\n",
    "            else:\n",
    "                if self.count >= self.patience - 1:\n",
    "                    next_lr = current_lr * self.factor\n",
    "                    self._update_lr(next_lr)\n",
    "                    self.count = 0\n",
    "                    self.stop_count += 1\n",
    "                    if v_loss < self.lowest_vloss:\n",
    "                        self.lowest_vloss = v_loss\n",
    "                else:\n",
    "                    self.count += 1\n",
    "        else:\n",
    "            monitor = 'val_loss'\n",
    "            pimprov = 0.0 if epoch == 0 else (self.lowest_vloss - v_loss) * 100 / self.lowest_vloss\n",
    "            if v_loss < self.lowest_vloss:\n",
    "                self.lowest_vloss = v_loss\n",
    "                self.best_weights = self._model.get_weights()\n",
    "                self.count = 0\n",
    "                self.stop_count = 0\n",
    "                self.best_epoch = epoch + 1\n",
    "            else:\n",
    "                if self.count >= self.patience - 1:\n",
    "                    next_lr = current_lr * self.factor\n",
    "                    self._update_lr(next_lr)\n",
    "                    self.stop_count += 1\n",
    "                    self.count = 0\n",
    "                else:\n",
    "                    self.count += 1\n",
    "                if acc > self.highest_tracc:\n",
    "                    self.highest_tracc = acc\n",
    "\n",
    "        msg = f'{epoch + 1:^8} {loss:^10.3f}{acc * 100:^9.2f}{v_loss:^9.5f}{v_acc * 100:^9.2f}{current_lr:^9.5f}{next_lr:^9.5f}{monitor:^11s}{pimprov:^10.2f}{duration:^8.2f}'\n",
    "        print(msg)\n",
    "\n",
    "        if self.stop_count > self.stop_patience - 1:\n",
    "            print(f\"\\nüõë Training halted at epoch {epoch + 1} ‚Äî no improvement after {self.stop_patience} learning rate adjustments.\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63be9ec2",
   "metadata": {},
   "source": [
    "## Code to test callback functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b302c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 00:54:31.139849: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2025-04-11 00:54:31.140040: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2025-04-11 00:54:31.140051: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2025-04-11 00:54:31.140470: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-04-11 00:54:31.140582: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Callback initialization complete.\n",
      " Epoch     Loss   Accuracy  V_loss    V_acc     LR     Next LR  Monitor  % Improv  Duration\n",
      "\n",
      "üîÅ Starting epoch 1\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 00:54:31.943183: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1       2.407     22.50   2.22072   40.00   0.00100  0.00100  accuracy     0.00     2.35  ss: 1.6198processing batch 2 of 6 - accuracy: 21.88% - loss: 2.28235 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 135ms/step - accuracy: 0.2234 - loss: 2.2054 - val_accuracy: 0.4000 - val_loss: 2.2207\n",
      "\n",
      "üîÅ Starting epoch 2\n",
      "Epoch 2/5\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.2510 - loss: 2.3673processing batch 2 of 6 - accuracy: 21.88% - loss: 2.36983    2       2.163     31.25   2.45557   10.00   0.00100  0.00100  accuracy    38.89     0.18  \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.2613 - loss: 2.3332 - val_accuracy: 0.1000 - val_loss: 2.4556\n",
      "\n",
      "üîÅ Starting epoch 3\n",
      "Epoch 3/5\n",
      "‚ö†Ô∏è Failed to update learning rate: 'str' object has no attribute 'name' - accuracy: 0.2734 - loss: 1.8801processing batch 2 of 6 - accuracy: 31.25% - loss: 1.90532 processing batch 5 of 6 - accuracy: 25.00% - loss: 1.73536 \n",
      "   3       1.735     25.00   1.81937   25.00   0.00100  0.00050  accuracy    -20.00    0.18  \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.2656 - loss: 1.8318 - val_accuracy: 0.2500 - val_loss: 1.8194\n",
      "\n",
      "üîÅ Starting epoch 4\n",
      "Epoch 4/5\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.1873 - loss: 1.8021processing batch 2 of 6 - accuracy: 25.00% - loss: 1.66776 ‚ö†Ô∏è Failed to update learning rate: 'str' object has no attribute 'name'\n",
      "   4       1.769     21.25   1.55720   25.00   0.00100  0.00050  accuracy    -32.00    0.17  \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.1915 - loss: 1.7966 - val_accuracy: 0.2500 - val_loss: 1.5572\n",
      "\n",
      "üîÅ Starting epoch 5\n",
      "Epoch 5/5\n",
      "‚ö†Ô∏è Failed to update learning rate: 'str' object has no attribute 'name' - accuracy: 0.2617 - loss: 1.5943processing batch 2 of 6 - accuracy: 21.88% - loss: 1.62058 processing batch 5 of 6 - accuracy: 23.75% - loss: 1.66769 \n",
      "   5       1.668     23.75   1.52533   30.00   0.00100  0.00050  accuracy    -24.00    0.18  \n",
      "\n",
      "üõë Training halted at epoch 5 ‚Äî no improvement after 3 learning rate adjustments.\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.2536 - loss: 1.6188 - val_accuracy: 0.3000 - val_loss: 1.5253\n",
      "\n",
      "Training time: 0h 0m 3.10s\n",
      "‚úÖ Best weights restored.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x317fb1970>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Assume your callback class is already defined above as MyCallback\n",
    "\n",
    "# 1. Create dummy image data (100 samples of 32x32 RGB images)\n",
    "X_dummy = np.random.rand(100, 32, 32, 3).astype(np.float32)\n",
    "\n",
    "# 2. Create dummy labels (4 classes)\n",
    "y_dummy = np.random.randint(0, 4, 100)\n",
    "\n",
    "# 3. Build a simple model\n",
    "model = keras.Sequential([\n",
    "    layers.Input(shape=(32, 32, 3)),\n",
    "    layers.Conv2D(16, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "# 4. Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 5. Create your custom callback instance\n",
    "cb = MyCallback(model=model, epochs=5, batches=X_dummy.shape[0] // 16)\n",
    "\n",
    "# 6. Train with the dummy data\n",
    "model.fit(X_dummy, y_dummy,\n",
    "          epochs=5,\n",
    "          batch_size=16,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[cb])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5f5ad0",
   "metadata": {},
   "source": [
    "## Data Preparation functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "757661c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_zip_files(zip_dir, extract_dir):\n",
    "    \"\"\"\n",
    "    Extract all zip files containing the image data\n",
    "    \"\"\"\n",
    "    # create extract directory if it doesn't exist\n",
    "    os.makedirs(extract_dir, exist_ok=True)\n",
    "    \n",
    "    # list of expected zip files\n",
    "    expected_zips = ['healthy.zip', 'cocci.zip', 'salmo.zip', 'ncd.zip']\n",
    "    \n",
    "    for zip_file in expected_zips:\n",
    "        zip_path = os.path.join(zip_dir, zip_file)\n",
    "        if os.path.exists(zip_path):\n",
    "            print(f\"Extracting {zip_file}...\")\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                # extract to a subfolder named after the class (removing .zip extension)\n",
    "                class_name = zip_file.split('.')[0]\n",
    "                class_dir = os.path.join(extract_dir, class_name)\n",
    "                os.makedirs(class_dir, exist_ok=True)\n",
    "                zip_ref.extractall(class_dir)\n",
    "            print(f\"Extracted {zip_file} to {class_dir}\")\n",
    "        else:\n",
    "            print(f\"Warning: {zip_file} not found in {zip_dir}\")\n",
    "\n",
    "def create_csv_from_directory(data_dir, output_csv_path):\n",
    "    \"\"\"\n",
    "    Create a CSV file with image paths and labels from directory structure\n",
    "    \"\"\"\n",
    "    filepaths = []\n",
    "    labels = []\n",
    "    \n",
    "    # Iterate through class directories\n",
    "    for class_name in os.listdir(data_dir):\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        if os.path.isdir(class_dir):\n",
    "            # Iterate through images in the class directory\n",
    "            for image_file in os.listdir(class_dir):\n",
    "                if image_file.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    # Use relative paths so it works in different environments\n",
    "                    image_path = os.path.join(class_name, image_file)\n",
    "                    filepaths.append(image_path)\n",
    "                    labels.append(class_name)\n",
    "    \n",
    "\n",
    "    df = pd.DataFrame({'filepaths': filepaths, 'labels': labels})\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Created CSV file with {len(df)} images\")\n",
    "    return df\n",
    "\n",
    "def split_data(data_dir, csv_path):\n",
    "    \"\"\"\n",
    "    Split the data into train, validation, and test sets\n",
    "    \"\"\"\n",
    "\n",
    "    if os.path.exists(csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df.columns = ['filepaths', 'labels']\n",
    "    else:\n",
    "        df = create_csv_from_directory(data_dir, csv_path)\n",
    "    \n",
    "    # Add full paths\n",
    "    df['filepaths'] = df['filepaths'].apply(lambda x: os.path.join(data_dir, x))\n",
    "    \n",
    "    # Create train df\n",
    "    strat = df['labels']\n",
    "    train_df, dummy_df = train_test_split(df, train_size=0.8, shuffle=True, random_state=123, stratify=strat)\n",
    "    \n",
    "    # Valid and test dataframe\n",
    "    strat = dummy_df['labels']\n",
    "    valid_df, test_df = train_test_split(dummy_df, train_size=0.5, shuffle=True, random_state=123, stratify=strat)\n",
    "    \n",
    "    return train_df, valid_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda8050c",
   "metadata": {},
   "source": [
    "## Data Generation Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4483ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gens(train_df, valid_df, test_df, batch_size, img_size=(160, 160)):\n",
    "    \"\"\"\n",
    "    Create image data generators for train, validation, and test sets\n",
    "    \"\"\"\n",
    "    channels = 3\n",
    "    color = 'rgb'\n",
    "    \n",
    "    # Calculate test batch size\n",
    "    ts_length = len(test_df)\n",
    "    test_batch_size = max(sorted([ts_length // n for n in range(1, ts_length + 1) \n",
    "                                  if ts_length % n == 0 and ts_length/n <= 80]))\n",
    "    \n",
    "    # Function for preprocessing\n",
    "    def scalar(img):\n",
    "        return img\n",
    "    \n",
    "    # Create generators with augmentation for training\n",
    "    tr_gen = ImageDataGenerator(\n",
    "        preprocessing_function=scalar,\n",
    "        horizontal_flip=True,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        brightness_range=[0.8, 1.2],\n",
    "        zoom_range=0.2\n",
    "    )\n",
    "    \n",
    "    ts_gen = ImageDataGenerator(preprocessing_function=scalar)\n",
    "    \n",
    "    # Flow from dataframes\n",
    "    train_gen = tr_gen.flow_from_dataframe(\n",
    "        train_df, x_col='filepaths', y_col='labels', target_size=img_size,\n",
    "        class_mode='categorical', color_mode=color, shuffle=True, batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    valid_gen = ts_gen.flow_from_dataframe(\n",
    "        valid_df, x_col='filepaths', y_col='labels', target_size=img_size,\n",
    "        class_mode='categorical', color_mode=color, shuffle=True, batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    test_gen = ts_gen.flow_from_dataframe(\n",
    "        test_df, x_col='filepaths', y_col='labels', target_size=img_size,\n",
    "        class_mode='categorical', color_mode=color, shuffle=False, batch_size=test_batch_size\n",
    "    )\n",
    "    \n",
    "    return train_gen, valid_gen, test_gen\n",
    "\n",
    "def show_images(gen):\n",
    "    \"\"\"\n",
    "    Show sample images from the generator\n",
    "    \"\"\"\n",
    "    g_dict = gen.class_indices\n",
    "    classes = list(g_dict.keys())\n",
    "    images, labels = next(gen)\n",
    "    \n",
    "    length = len(labels)\n",
    "    sample = min(length, 25)\n",
    "    \n",
    "    plt.figure(figsize=(20, 20))\n",
    "    for i in range(sample):\n",
    "        plt.subplot(5, 5, i + 1)\n",
    "        image = images[i] / 255\n",
    "        plt.imshow(image)\n",
    "        index = np.argmax(labels[i])\n",
    "        class_name = classes[index]\n",
    "        plt.title(class_name, color='blue', fontsize=12)\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f265e3",
   "metadata": {},
   "source": [
    "## Visualization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc9aac05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(hist):\n",
    "    \"\"\"\n",
    "    Plot training history\n",
    "    \"\"\"\n",
    "    tr_acc = hist.history['accuracy']\n",
    "    tr_loss = hist.history['loss']\n",
    "    val_acc = hist.history['val_accuracy']\n",
    "    val_loss = hist.history['val_loss']\n",
    "    \n",
    "    index_loss = np.argmin(val_loss)\n",
    "    val_lowest = val_loss[index_loss]\n",
    "    index_acc = np.argmax(val_acc)\n",
    "    acc_highest = val_acc[index_acc]\n",
    "    \n",
    "    Epochs = [i+1 for i in range(len(tr_acc))]\n",
    "    loss_label = f'best epoch= {str(index_loss + 1)}'\n",
    "    acc_label = f'best epoch= {str(index_acc + 1)}'\n",
    "    \n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.style.use('fivethirtyeight')\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(Epochs, tr_loss, 'r', label='Training loss')\n",
    "    plt.plot(Epochs, val_loss, 'g', label='Validation loss')\n",
    "    plt.scatter(index_loss + 1, val_lowest, s=150, c='blue', label=loss_label)\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(Epochs, tr_acc, 'r', label='Training Accuracy')\n",
    "    plt.plot(Epochs, val_acc, 'g', label='Validation Accuracy')\n",
    "    plt.scatter(index_acc + 1, acc_highest, s=150, c='blue', label=acc_label)\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    \n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print('Normalized Confusion Matrix')\n",
    "    else:\n",
    "        print('Confusion Matrix, Without Normalization')\n",
    "    \n",
    "    print(cm)\n",
    "    \n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, cm[i, j], horizontalalignment='center', \n",
    "                     color='white' if cm[i, j] > thresh else 'black')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d75bd7",
   "metadata": {},
   "source": [
    "## 7) Model building and evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e4dc9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(class_count, img_shape=(160, 160, 3)):\n",
    "    \"\"\"\n",
    "    Build the model (using EfficientNetB0 instead of B3 for better performance on Mac M2)\n",
    "    \"\"\"\n",
    "    # Create pre-trained model - using B0 which is smaller than B3\n",
    "    base_model = tf.keras.applications.EfficientNetB0(\n",
    "        include_top=False, \n",
    "        weights=\"imagenet\", \n",
    "        input_shape=img_shape, \n",
    "        pooling='max'\n",
    "    )\n",
    "    \n",
    "    # Freeze base model layers\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001),\n",
    "        Dense(128, kernel_regularizer=regularizers.l2(0.016),  # Fixed: removed 'l=' parameter\n",
    "              activity_regularizer=regularizers.l1(0.006),\n",
    "              bias_regularizer=regularizers.l1(0.006), activation='relu'),\n",
    "        Dropout(rate=0.45, seed=123),\n",
    "        Dense(class_count, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        Adamax(learning_rate=0.001), \n",
    "        loss='categorical_crossentropy', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_model(model, train_gen, valid_gen, epochs=20):\n",
    "    \"\"\"\n",
    "    Train the model with custom callback\n",
    "    \"\"\"\n",
    "    patience = 1\n",
    "    stop_patience = 3\n",
    "    threshold = 0.9\n",
    "    factor = 0.5\n",
    "    ask_epoch = 5\n",
    "    batch_size = BATCH_SIZE\n",
    "    batches = int(np.ceil(len(train_gen.labels) / batch_size))\n",
    "    \n",
    "    callbacks = [MyCallback(\n",
    "        model=model, patience=patience, stop_patience=stop_patience, \n",
    "        threshold=threshold, factor=factor, batches=batches, \n",
    "        epochs=epochs, ask_epoch=ask_epoch\n",
    "    )]\n",
    "    \n",
    "    history = model.fit(\n",
    "        x=train_gen, epochs=epochs, verbose=0, callbacks=callbacks,\n",
    "        validation_data=valid_gen, validation_steps=None, shuffle=False\n",
    "    )\n",
    "    \n",
    "    return history\n",
    "\n",
    "def evaluate_model(model, test_gen):\n",
    "    \"\"\"\n",
    "    Evaluate model on test set and display results\n",
    "    \"\"\"\n",
    "    # Reset generator to start\n",
    "    test_gen.reset()\n",
    "    \n",
    "    # Predict\n",
    "    predictions = model.predict(test_gen, steps=len(test_gen), verbose=1)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Get true classes\n",
    "    true_classes = test_gen.classes\n",
    "    class_labels = list(test_gen.class_indices.keys())\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(true_classes, predicted_classes)\n",
    "    plot_confusion_matrix(cm, class_labels, title='Confusion Matrix')\n",
    "    \n",
    "    # Print classification report\n",
    "    print('\\nClassification Report')\n",
    "    print(classification_report(true_classes, predicted_classes, target_names=class_labels))\n",
    "    \n",
    "    # Calculate overall accuracy\n",
    "    accuracy = np.sum(predicted_classes == true_classes) / len(true_classes)\n",
    "    print(f'\\nOverall Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf18a1e1",
   "metadata": {},
   "source": [
    "## 8) Data preparation and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a62c919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Extract zip files if needed\n",
      "Warning: healthy.zip not found in chicken_feces_Zips\n",
      "Warning: cocci.zip not found in chicken_feces_Zips\n",
      "Warning: salmo.zip not found in chicken_feces_Zips\n",
      "Warning: ncd.zip not found in chicken_feces_Zips\n"
     ]
    }
   ],
   "source": [
    "# Set your directories here\n",
    "ZIP_DIR = \"chicken_feces_Zips\"  \n",
    "EXTRACT_DIR = \"extracted_images\"\n",
    "CSV_PATH = \"poultry_data.csv\"\n",
    "\n",
    "# Step 1: Extract zip files if needed\n",
    "print(\"Step 1: Extract zip files if needed\")\n",
    "if not os.path.exists(EXTRACT_DIR) or len(os.listdir(EXTRACT_DIR)) < 4:\n",
    "    extract_zip_files(ZIP_DIR, EXTRACT_DIR)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed6c4f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prepare data\n",
      "Created CSV file with 0 images (0 total)\n",
      "No images found! Make sure the zip files were properly extracted with image files inside.\n",
      "Check the following directories:\n",
      "  ‚Ä¢ extracted_images/healthy does not exist\n",
      "  ‚Ä¢ extracted_images/cocci does not exist\n",
      "  ‚Ä¢ extracted_images/salmo does not exist\n",
      "  ‚Ä¢ extracted_images/ncd does not exist\n"
     ]
    }
   ],
   "source": [
    "# Set directories\n",
    "ZIP_DIR = \"chicken_feces_Zips\"  \n",
    "EXTRACT_DIR = \"extracted_images\"\n",
    "CSV_PATH = \"poultry_data.csv\"\n",
    "\n",
    "\n",
    "print(\"\\nPrepare data\")\n",
    "def create_fixed_csv():\n",
    "    filepaths = []\n",
    "    labels = []\n",
    "    \n",
    "    # Function to recursively find all image files in a directory and its subdirectories\n",
    "    def find_images_recursive(base_dir, class_name):\n",
    "        count = 0\n",
    "        for root, dirs, files in os.walk(base_dir):\n",
    "            for file in files:\n",
    "                if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    filepaths.append(file_path)\n",
    "                    labels.append(class_name)\n",
    "                    count += 1\n",
    "        return count\n",
    "    \n",
    "    # Process each class directory\n",
    "    total_count = 0\n",
    "    for class_name in ['healthy', 'cocci', 'salmo', 'ncd']:\n",
    "        class_dir = os.path.join(EXTRACT_DIR, class_name)\n",
    "        if os.path.isdir(class_dir):\n",
    "            print(f\"Processing {class_name} directory...\")\n",
    "            found = find_images_recursive(class_dir, class_name)\n",
    "            print(f\"  Found {found} images in {class_name}\")\n",
    "            total_count += found\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({'filepaths': filepaths, 'labels': labels})\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(CSV_PATH, index=False)\n",
    "    print(f\"Created CSV file with {len(df)} images ({total_count} total)\")\n",
    "    return df\n",
    "\n",
    "\n",
    "full_df = create_fixed_csv()\n",
    "\n",
    "if len(full_df) > 0:\n",
    "    # Split into train and temp dfs\n",
    "    strat = full_df['labels']\n",
    "    train_df, dummy_df = train_test_split(full_df, train_size=0.8, shuffle=True, random_state=123, stratify=strat)\n",
    "    \n",
    "    # Split temp into valid and test\n",
    "    strat = dummy_df['labels']\n",
    "    valid_df, test_df = train_test_split(dummy_df, train_size=0.5, shuffle=True, random_state=123, stratify=strat)\n",
    "    \n",
    "    print(f\"Training samples: {len(train_df)}\")\n",
    "    print(f\"Validation samples: {len(valid_df)}\")\n",
    "    print(f\"Test samples: {len(test_df)}\")\n",
    "    \n",
    "\n",
    "    print(\"\\nCreate generators\")\n",
    "    train_gen, valid_gen, test_gen = create_gens(train_df, valid_df, test_df, BATCH_SIZE, img_size=IMG_SIZE)\n",
    "    \n",
    "    print(\"\\nBuilding the model\")\n",
    "    class_count = len(train_gen.class_indices)\n",
    "    model = build_model(class_count, img_shape=IMG_SHAPE)\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    print(\"\\nTrain the model\")\n",
    "    EPOCHS = 20  # Set your desired epochs\n",
    "    history = train_model(model, train_gen, valid_gen, epochs=EPOCHS)\n",
    "\n",
    "\n",
    "    print(\"\\nPlot training history\")\n",
    "    plot_training(history)\n",
    "\n",
    "\n",
    "    print(\"\\nEvaluate model on test set\")\n",
    "    evaluate_model(model, test_gen)\n",
    "\n",
    "\n",
    "    print(\"\\n Save the model\")\n",
    "    model.save('poultry_disease_model.h5')\n",
    "    print(\"Model saved successfully!\")\n",
    "\n",
    "else:\n",
    "    print(\"No images found! Make sure the zip files were properly extracted with image files inside.\")\n",
    "    print(\"Check the following directories:\")\n",
    "    for class_name in ['healthy', 'cocci', 'salmo', 'ncd']:\n",
    "        path = os.path.join(EXTRACT_DIR, class_name)\n",
    "        if os.path.exists(path):\n",
    "            print(f\"  ‚Ä¢ {path} exists\")\n",
    "            # List some files in this directory\n",
    "            files = os.listdir(path)\n",
    "            print(f\"    - Contains {len(files)} files/directories\")\n",
    "            if files:\n",
    "                print(f\"    - Sample items: {files[:5]}\")\n",
    "        else:\n",
    "            print(f\"  ‚Ä¢ {path} does not exist\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-metal-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
